{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network from Scratch using NumPy"
      ],
      "metadata": {
        "id": "pJqQ0d3bP4Mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Libraries"
      ],
      "metadata": {
        "id": "W5TGOun7P-Y3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GrJoc7SIOkm8"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Helper Functions for Activation and Derivatives\n",
        "We’ll use the sigmoid activation function and its derivative, which are essential for both the forward and backward passes."
      ],
      "metadata": {
        "id": "1bz_aPMVQn9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n"
      ],
      "metadata": {
        "id": "oaRbUTYdQpU_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Dummy Data\n",
        "To demonstrate the network, we’ll create a small dataset with 3 input features and 2 output classes."
      ],
      "metadata": {
        "id": "Q0mOtaV6RQA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "# Input data (4 samples with 3 features each)\n",
        "X = np.array([[0, 0, 1],\n",
        "              [1, 1, 1],\n",
        "              [1, 0, 1],\n",
        "              [0, 1, 1]])\n",
        "\n",
        "# Target output (4 samples with 2 classes each)\n",
        "y = np.array([[0, 1],\n",
        "              [1, 0],\n",
        "              [1, 0],\n",
        "              [0, 1]])\n"
      ],
      "metadata": {
        "id": "meU84aiwQrEm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Network Architecture and Initialize Parameters\n",
        "Here, we set the input size, hidden layer size, and output size, then initialize weights and biases randomly."
      ],
      "metadata": {
        "id": "fTFDYs_-RWCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Network architecture\n",
        "input_size = X.shape[1]    # 3 input features\n",
        "hidden_size = 4            # 4 neurons in the hidden layer\n",
        "output_size = y.shape[1]   # 2 output classes\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "W1 = np.random.rand(input_size, hidden_size) - 0.5  # Weights for input to hidden layer\n",
        "b1 = np.random.rand(1, hidden_size) - 0.5           # Bias for hidden layer\n",
        "W2 = np.random.rand(hidden_size, output_size) - 0.5 # Weights for hidden to output layer\n",
        "b2 = np.random.rand(1, output_size) - 0.5           # Bias for output layer\n"
      ],
      "metadata": {
        "id": "y9zMT2j6RTCm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Hyperparameters\n",
        "Define the learning rate and the number of epochs for training."
      ],
      "metadata": {
        "id": "ukDuYwotRbJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "epochs = 10000\n"
      ],
      "metadata": {
        "id": "vCIt19nHRZCH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Implement the Training Loop\n",
        "This loop performs both the forward and backward passes, updating weights and biases."
      ],
      "metadata": {
        "id": "162-UKF_RgEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    # Step 1: Compute hidden layer activations\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "\n",
        "    # Step 2: Compute output layer activations\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    # Compute loss (Mean Squared Error)\n",
        "    loss = np.mean((y - A2) ** 2)\n",
        "\n",
        "    # Backward pass\n",
        "    # Step 1: Output layer error and gradient\n",
        "    dZ2 = A2 - y\n",
        "    dW2 = np.dot(A1.T, dZ2) * sigmoid_derivative(A2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    # Step 2: Hidden layer error and gradient\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * sigmoid_derivative(A1)\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAgWSYGrRjD_",
        "outputId": "09fd78a9-4a02-4dd9-c728-abf6dd0df319"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2497\n",
            "Epoch 1000, Loss: 0.0021\n",
            "Epoch 2000, Loss: 0.0008\n",
            "Epoch 3000, Loss: 0.0005\n",
            "Epoch 4000, Loss: 0.0004\n",
            "Epoch 5000, Loss: 0.0003\n",
            "Epoch 6000, Loss: 0.0002\n",
            "Epoch 7000, Loss: 0.0002\n",
            "Epoch 8000, Loss: 0.0002\n",
            "Epoch 9000, Loss: 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Trained Model\n",
        "After training, we test the model by making predictions."
      ],
      "metadata": {
        "id": "XumNXah9RlBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPredictions after training:\")\n",
        "print(A2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NCH98qIRoVX",
        "outputId": "cf900e84-aec2-40ed-ea37-9ed65cc6ae57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions after training:\n",
            "[[0.01123573 0.98844917]\n",
            " [0.98880527 0.01151319]\n",
            " [0.98911045 0.01124646]\n",
            " [0.01085271 0.98878362]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "We implemented a simple neural network from scratch using NumPy. We covered:\n",
        "\n",
        "- Forward Pass: Calculating outputs using weights, biases, and activation functions.\n",
        "- Loss Calculation: Using Mean Squared Error to evaluate performance.\n",
        "- Backward Pass: Computing gradients to update weights and biases with gradient descent.\n"
      ],
      "metadata": {
        "id": "bybQ4OOjRu43"
      }
    }
  ]
}